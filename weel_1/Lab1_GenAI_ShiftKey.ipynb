{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMlKO2xLhCSGQmcJaakAG+o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shiftkey-labs/GenAI-Course/blob/main/weel_1/Lab1_GenAI_ShiftKey.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generative AI: Introduction and Hands-On with Hugging Face\n",
        "\n",
        "**Certification by ShiftKey Labs**  \n",
        "**Content created by Vansh Sood**\n",
        "\n",
        "This notebook is designed to give you hands-on experience with the tools and concepts you'll need to work with generative AI models. We'll be using Google Colab to run Python code, explore some basic concepts, and dive into working with pre-trained models from Hugging Face.\n",
        "\n",
        "## Sections Covered\n",
        "1. Introduction and Overview\n",
        "2. Playing Around with Google Colab\n",
        "3. Coding in Python: Getting Comfortable with Google Colab\n",
        "4. Setting Up Libraries from Hugging Face\n",
        "5. Importing Necessary Modules\n",
        "6. Understanding the Imports\n",
        "7. Loading a Pre-trained Model using Transformers\n",
        "8. Tokenizing Input and Model Inference\n",
        "9. Outputs and Prompts\n",
        "\n",
        "Let's begin!"
      ],
      "metadata": {
        "id": "E1vsD2qwmosN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Introduction and Overview\n",
        "\n",
        "In this notebook, we'll walk through setting up your environment in Google Colab, coding in Python to get familiar with the platform, and working with pre-trained Transformer models from Hugging Face.\n",
        "\n",
        "By the end of this session, you'll have a better understanding of how to use Google Colab, work with Python code in the cloud, and start using powerful AI models from Hugging Face for various natural language processing (NLP) tasks."
      ],
      "metadata": {
        "id": "wlNZ_jF4vF78"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Playing Around with Google Colab\n",
        "\n",
        "Google Colab is a great tool for running Python code in the cloud, with access to free GPUs. Before diving into the main content, let's explore the Colab environment.\n",
        "\n",
        "### Task: Simple Arithmetic Operations\n",
        "\n",
        "We'll start with something simpleâ€”basic arithmetic in Python. This helps you get comfortable with running code cells in Colab."
      ],
      "metadata": {
        "id": "1y7LlruBmtbe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's do some basic arithmetic\n",
        "addition = 5 + 3\n",
        "multiplication = 7 * 6\n",
        "division = 16 / 4\n",
        "subtraction = 10 - 2\n",
        "\n",
        "# Print the results\n",
        "addition, multiplication, division, subtraction"
      ],
      "metadata": {
        "id": "WRwhts31vXXy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Coding in Python: Getting Comfortable with Google Colab\n",
        "\n",
        "Now that we've done some basic arithmetic, let's try something a bit more complex. We'll write a simple Python function and see how it works.\n",
        "\n",
        "### Task: Write a Function to Check Even or Odd Numbers\n",
        "\n",
        "Let's write a Python function that checks whether a number is even or odd."
      ],
      "metadata": {
        "id": "cb5UngrXvZv9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to check if a number is even or odd\n",
        "def check_even_odd(number):\n",
        "    if number % 2 == 0:\n",
        "        return f\"{number} is even.\"\n",
        "    else:\n",
        "        return f\"{number} is odd.\"\n",
        "\n",
        "# Test the function with a few examples\n",
        "check_even_odd(7), check_even_odd(10)"
      ],
      "metadata": {
        "id": "08BLfkYTvbs3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Setting Up Libraries from Hugging Face\n",
        "\n",
        "Now that we're comfortable with the basics, let's set up the libraries we'll need to work with pre-trained models from Hugging Face.\n",
        "\n",
        "### Task: Install the Hugging Face Transformers Library\n",
        "\n",
        "Hugging Face provides the `transformers` library, which makes it easy to use state-of-the-art NLP models. We'll start by installing this library.\n"
      ],
      "metadata": {
        "id": "ABpOU2RnvhlB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the Hugging Face Transformers library\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "10V6dEz2vjws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Importing Necessary Modules\n",
        "\n",
        "After installing the library, the next step is to import the necessary modules that we'll use to work with models and tokenizers.\n",
        "\n",
        "### Task: Import Modules from Hugging Face"
      ],
      "metadata": {
        "id": "O4ykbj3uvn0q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary modules from Hugging Face\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "print(\"Modules imported successfully.\")"
      ],
      "metadata": {
        "id": "QNzTepybvqr6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explanation\n",
        "\n",
        "In this section, we imported:\n",
        "- `AutoTokenizer`: A class that provides all the necessary tools to preprocess text data for the model.\n",
        "- `AutoModelForSequenceClassification`: A class that loads a pre-trained model for sequence classification tasks.\n",
        "\n",
        "These imports are critical for tokenizing input data and loading the pre-trained models."
      ],
      "metadata": {
        "id": "LerY27xNvuAZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device = 0 if torch.cuda.is_available() else -1"
      ],
      "metadata": {
        "id": "hr8TCbYK21ai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Loading a Pre-trained Model using Transformers\n",
        "\n",
        "We will now load a pre-trained model using the Hugging Face `transformers` library. This model will be used for a sequence classification task.\n",
        "\n",
        "### Task: Load a Pre-trained BERT Model\n",
        "\n",
        "We'll load the BERT model, which is widely used in NLP tasks."
      ],
      "metadata": {
        "id": "q80H1xYkvyn_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a pre-trained BERT model and its tokenizer\n",
        "model_name = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "print(f\"Model {model_name} loaded successfully.\")"
      ],
      "metadata": {
        "id": "MVakRpUAv1Zl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Tokenizing Input and Model Inference\n",
        "\n",
        "Now that we've loaded the model and tokenizer, the next step is to prepare some text input for the model. We'll tokenize the input and then pass it through the model to get a prediction.\n",
        "\n",
        "### Task: Tokenize a Sentence and Get Model Output"
      ],
      "metadata": {
        "id": "gLh6QXYjv3nV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize an example input\n",
        "input_text = \"Generative AI is transforming technology.\"\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
        "\n",
        "# Perform inference with the model\n",
        "outputs = model(**inputs)\n",
        "\n",
        "# Print the outputs\n",
        "outputs"
      ],
      "metadata": {
        "id": "sS_HlCQXv5fh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explanation\n",
        "\n",
        "In this code:\n",
        "- We defined an input sentence `Generative AI is transforming technology`.\n",
        "- The `tokenizer` converts this sentence into the format required by the BERT model. The `return_tensors=\"pt\"` argument specifies that the output should be PyTorch tensors.\n",
        "- We then pass the tokenized inputs to the model using `model(**inputs)`, which performs the inference.\n",
        "- The outputs are printed, which typically include logits (raw predictions) from the model.\n",
        "\n",
        "This step demonstrates how you can input text into a pre-trained model and get a prediction.\n"
      ],
      "metadata": {
        "id": "zyOQAiZDv7qn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Asking Different Questions to the NLP Model\n",
        "\n",
        "In this section, we will use the `Flan-T5 base` model to perform various NLP tasks, including translation and summarization. While the model can handle these tasks, the results might not always be perfect. We'll explore how to improve them using prompt engineering in the next session.\n",
        "\n",
        "### Task: Text Translation\n",
        "Let's use the `Flan-T5 base` model to translate a sentence from English to another language.\n"
      ],
      "metadata": {
        "id": "jio2IHjSw4mp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Flan-T5 base model for translation\n",
        "from transformers import pipeline\n",
        "\n",
        "# Create a pipeline for translation using Flan-T5 base\n",
        "translator = pipeline(\"translation_en_to_fr\", model=\"google/flan-t5-base\")\n",
        "\n",
        "# Translate a sentence from English to French\n",
        "translation = translator(\"Generative AI is transforming the world of technology.\")\n",
        "print(\"Translation:\", translation[0]['translation_text'])"
      ],
      "metadata": {
        "id": "dH2yz4j-w8x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explanation\n",
        "\n",
        "Here, we used the `Flan-T5 base` model via the `pipeline` function for translation. We translated the sentence \"Generative AI is transforming the world of technology\" from English to French using the `google/flan-t5-base` model.\n",
        "\n",
        "The translation may be accurate for simple sentences, but more complex sentences could result in less precise translations. We'll work on improving this in the next session."
      ],
      "metadata": {
        "id": "-iMdviWEyyeA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task: Text Summarization\n",
        "\n",
        "Next, let's use the same `Flan-T5 base` model for summarizing a longer text."
      ],
      "metadata": {
        "id": "PPgKhIdRyi9s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a pipeline for summarization using Flan-T5 base\n",
        "summarizer = pipeline(\"summarization\", model=\"google/flan-t5-base\", device=device)\n",
        "\n",
        "# Example paragraph to summarize\n",
        "text = \"\"\"\n",
        "summarize:\n",
        "Generative AI is a field within artificial intelligence focused on creating models that can generate new content.\n",
        "These models can produce text, images, music, and even code. Companies are increasingly investing in generative AI\n",
        "to develop innovative products and services, transforming industries and driving technological advancement.\n",
        "\"\"\"\n",
        "\n",
        "# Generate a summary of the paragraph\n",
        "summary = summarizer(text, max_length=50, min_length=25, do_sample=False)\n",
        "print(\"Summary:\", summary[0]['summary_text'])"
      ],
      "metadata": {
        "id": "lDGkd7m7yj1g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explanation\n",
        "\n",
        "In this section, we used the `Flan-T5 base` model for text summarization. The same model can be adapted for various tasks, showcasing its versatility.\n",
        "\n",
        "The summary provided is a concise version of the original text. As with translation, the summarization may not always capture all the nuances of the original content. We will explore how to refine these summaries using prompt engineering in the next session.\n"
      ],
      "metadata": {
        "id": "5gISV0cJyn27"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10. Conclusion and Preview for Next Session\n",
        "\n",
        "In this session, we've used the `Flan-T5 base` model to perform various NLP tasks such as translation and summarization. The `Flan-T5 base` model is powerful and flexible, but the results we obtained are not always perfect.\n",
        "\n",
        "In our next session, we will focus on **prompt engineering**, a technique that allows us to refine these outputs by optimizing the input prompts. By learning how to craft better prompts, we can significantly enhance the performance of these models.\n",
        "\n",
        "See you in the next session!\n"
      ],
      "metadata": {
        "id": "XwGlX5H5ypbF"
      }
    }
  ]
}